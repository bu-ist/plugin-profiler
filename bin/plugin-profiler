#!/usr/bin/env bash

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

# ── Colour helpers ──────────────────────────────────────────────────────────
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Colour

info()  { echo -e "${GREEN}[plugin-profiler]${NC} $*"; }
warn()  { echo -e "${YELLOW}[plugin-profiler]${NC} $*"; }
error() { echo -e "${RED}[plugin-profiler] ERROR:${NC} $*" >&2; }

# ── Prerequisite checks ─────────────────────────────────────────────────────
if ! command -v docker &>/dev/null; then
    error "Docker is not installed or not in PATH."
    exit 1
fi

if ! docker compose version &>/dev/null; then
    error "Docker Compose v2 is required ('docker compose' command not found)."
    exit 1
fi

# ── Usage ───────────────────────────────────────────────────────────────────
usage() {
    cat <<EOF
Usage: plugin-profiler analyze <plugin-path> [options]

Arguments:
  <plugin-path>        Path to the WordPress plugin directory

Options:
  --port <n>           Port for web UI (default: 9000)
  --llm <provider>     LLM provider: ollama, gemini, openai, deepseek (default: ollama)
  --model <name>       LLM model name (default: qwen2.5-coder:7b)
  --api-key <key>      API key for external LLM provider
  --no-descriptions    Skip LLM description generation
  --json-only          Output JSON only, do not start web server
  --output <dir>       Output directory inside container (default: /output)
  --help               Show this help message

Examples:
  plugin-profiler analyze ./my-plugin
  plugin-profiler analyze ./my-plugin --no-descriptions
  plugin-profiler analyze ./my-plugin --llm gemini --api-key sk-xxx
  plugin-profiler analyze ./my-plugin --port 8080 --json-only
EOF
}

# ── Parse arguments ─────────────────────────────────────────────────────────
if [[ $# -eq 0 ]] || [[ "$1" == "--help" ]] || [[ "$1" == "-h" ]]; then
    usage
    exit 0
fi

SUBCOMMAND="${1:-}"
shift || true

if [[ "$SUBCOMMAND" != "analyze" ]]; then
    error "Unknown subcommand: '$SUBCOMMAND'. Expected: analyze"
    usage
    exit 1
fi

PLUGIN_ARG="${1:-}"
shift || true

if [[ -z "$PLUGIN_ARG" ]]; then
    error "Plugin path is required."
    usage
    exit 1
fi

if [[ ! -d "$PLUGIN_ARG" ]]; then
    error "Plugin path does not exist or is not a directory: $PLUGIN_ARG"
    exit 1
fi

# Defaults
PORT=9000
LLM_PROVIDER=ollama
LLM_MODEL="qwen2.5-coder:7b"
API_KEY=""
NO_DESCRIPTIONS=false
JSON_ONLY=false
OUTPUT_DIR="/output"

# Parse remaining options
while [[ $# -gt 0 ]]; do
    case "$1" in
        --port)        PORT="$2";        shift 2 ;;
        --llm)         LLM_PROVIDER="$2"; shift 2 ;;
        --model)       LLM_MODEL="$2";   shift 2 ;;
        --api-key)     API_KEY="$2";     shift 2 ;;
        --no-descriptions) NO_DESCRIPTIONS=true; shift ;;
        --json-only)   JSON_ONLY=true;   shift ;;
        --output)      OUTPUT_DIR="$2";  shift 2 ;;
        --help|-h)     usage; exit 0 ;;
        *) error "Unknown option: $1"; usage; exit 1 ;;
    esac
done

# ── Resolve plugin path ─────────────────────────────────────────────────────
export PLUGIN_PATH
PLUGIN_PATH="$(realpath "$PLUGIN_ARG")"
export PORT

info "Plugin: $PLUGIN_PATH"
info "Port:   $PORT"
info "LLM:    $LLM_PROVIDER ($LLM_MODEL)"

# ── Build compose arguments ─────────────────────────────────────────────────
COMPOSE_ARGS=("--project-directory" "$PROJECT_DIR")
ANALYZER_ENV=(-e "LLM_PROVIDER=$LLM_PROVIDER" -e "LLM_MODEL=$LLM_MODEL")

if [[ -n "$API_KEY" ]]; then
    ANALYZER_ENV+=(-e "LLM_API_KEY=$API_KEY")
fi

ANALYZER_CMD=("/plugin")

if $NO_DESCRIPTIONS; then
    ANALYZER_CMD+=("--no-descriptions")
fi

if $JSON_ONLY; then
    ANALYZER_CMD+=("--json-only")
fi

ANALYZER_CMD+=("--output" "$OUTPUT_DIR")

# ── Determine LLM profile ───────────────────────────────────────────────────
USE_LLM_PROFILE=false
if ! $NO_DESCRIPTIONS && [[ "$LLM_PROVIDER" == "ollama" ]]; then
    USE_LLM_PROFILE=true
fi

PROFILE_ARGS=()
if $USE_LLM_PROFILE; then
    PROFILE_ARGS+=("--profile" "llm")
    info "Starting with Ollama LLM container..."
fi

# ── Build images ────────────────────────────────────────────────────────────
info "Building containers..."
docker compose "${COMPOSE_ARGS[@]}" ${PROFILE_ARGS[@]+"${PROFILE_ARGS[@]}"} build analyzer web

# ── Run analyzer ────────────────────────────────────────────────────────────
info "Running analysis..."
if ! docker compose "${COMPOSE_ARGS[@]}" ${PROFILE_ARGS[@]+"${PROFILE_ARGS[@]}"} run --rm \
    ${ANALYZER_ENV[@]+"${ANALYZER_ENV[@]}"} \
    analyzer "${ANALYZER_CMD[@]}"; then
    error "Analysis failed. Check logs above."
    exit 1
fi

info "Analysis complete."

# ── Start web server (unless json-only) ─────────────────────────────────────
if ! $JSON_ONLY; then
    info "Starting web server on port $PORT..."
    docker compose "${COMPOSE_ARGS[@]}" up -d web

    URL="http://localhost:$PORT"
    info "Graph available at: $URL"

    # Open browser
    if command -v open &>/dev/null; then
        open "$URL"
    elif command -v xdg-open &>/dev/null; then
        xdg-open "$URL"
    else
        warn "Could not auto-open browser. Visit: $URL"
    fi
fi
